{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTrain: (90, 3)\n",
      "XTest: (51, 3)\n",
      "TTrain: (3, 90)\n",
      "TTest: (3, 51)\n"
     ]
    }
   ],
   "source": [
    "train = np.loadtxt('iris/iris-train.txt')\n",
    "test = np.loadtxt('iris/iris-test.txt')\n",
    "XTrain = np.append(train[:,1:],np.ones((train.shape[0],1)),axis=1)\n",
    "yTrain = train[:,0]\n",
    "TTrain = LabelBinarizer().fit_transform(yTrain).T\n",
    "XTest = np.append(test[:,1:],np.ones((test.shape[0],1)),axis=1)\n",
    "yTest = test[:,0]\n",
    "TTest = LabelBinarizer().fit_transform(yTest).T\n",
    "print(\"XTrain:\",XTrain.shape)\n",
    "print(\"XTest:\",XTest.shape)\n",
    "print(\"TTrain:\",TTrain.shape)\n",
    "print(\"TTest:\",TTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def musicMSE(pred, gt):\n",
    "#     mse = (np.square(np.rint(pred)-gt)).mean()\n",
    "#     return mse\n",
    "# print(musicMSE(np.zeros(3),np.ones(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(X,W): #will be kxn (3 classes by n examples)\n",
    "    mat = np.matmul(W.T,X)\n",
    "    num = np.exp(mat-np.max(mat))\n",
    "    return num/np.sum(num,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(XTrain,TTrain,XTest,TTest,alpha,lr,epochs=100,momentum=0,batch_size = 128):\n",
    "    W = np.zeros((TTrain.shape[0],XTrain.shape[1]))\n",
    "    dW = np.zeros((TTrain.shape[0],XTrain.shape[1]))\n",
    "    nTrain, featCount = XTrain.shape\n",
    "    nTest = XTest.shape[0]\n",
    "\n",
    "    def P(X,W): #will be nxk (3 classes by n examples)\n",
    "        mat = np.matmul(X,W)\n",
    "        num = np.exp(np.exp(mat-np.max(mat)))\n",
    "        return num/np.sum(num,axis=0) \n",
    "    def getgrad(X,T,W):\n",
    "        return alpha*W + (np.matmul((P(X,W).T-T),X))\n",
    "    def getlosses(XTrain,XTest,TTrain,TTest,W):\n",
    "        train_loss = -np.sum(np.matmul(TTrain,np.log(P(XTrain,W))))/nTrain\n",
    "        test_loss = -np.sum(np.matmul(TTest,np.log(P(XTest,W))))/nTest\n",
    "        return train_loss,test_loss\n",
    "    def getaccuracies(X,T,W):\n",
    "        guesses = np.argmax(P(X,W),axis=1)\n",
    "        truths = np.argmax(T,axis=0)\n",
    "        acc = 0\n",
    "        for i in range(len(guesses)):\n",
    "            if guesses[i] == truths[i]:\n",
    "                acc+=1\n",
    "        return acc/len(guesses)\n",
    "    XTrain2 = np.copy(XTrain)\n",
    "    XTrain2[:,:-1] = 2*(XTrain2[:,:-1]-0.5)\n",
    "    XTest2 = np.copy(XTest)\n",
    "    XTest2[:,:-1] = 2*(XTest2[:,:-1]-0.5)\n",
    "    TTrain2 = np.copy(TTrain)\n",
    "    TTest2 = np.copy(TTest)\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    for i in range(epochs):\n",
    "        #shuffle train data:\n",
    "        p = random.permutation(XTrain2.shape[0])\n",
    "        XTrain2 = XTrain2[p]\n",
    "        TTrain2 = TTrain2[:,p]\n",
    "        for b in range(math.ceil(XTrain2.shape[0]/batch_size)):\n",
    "            if(batch_size*(b+1) < XTrain2.shape[0]):\n",
    "                batchXTrain = np.copy(XTrain2[batch_size*b:batch_size*(b+1)])\n",
    "                batchTTrain = np.copy(TTrain2[:,batch_size*b:batch_size*(b+1)])\n",
    "            else:\n",
    "                batchXTrain = np.copy(XTrain2[batch_size*b:-1])\n",
    "                batchTTrain = np.copy(TTrain2[:,batch_size*b:-1])\n",
    "            grad = getgrad(batchXTrain,batchTTrain,W)\n",
    "            # print(\"train:\",batchTTrain[:,:5])\n",
    "            # print(\"Grad:\",grad)\n",
    "            dW = lr*dW + grad\n",
    "            # print(\"dW:\",dW)\n",
    "            W -= dW\n",
    "            # print(\"W:\",W)\n",
    "            # print(W)\n",
    "        train_loss,test_loss = getlosses(XTrain2,XTest2,TTrain2,TTest2,W)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(getaccuracies(XTrain2,TTrain2,W))\n",
    "        test_accuracies.append(getaccuracies(XTest2,TTest2,W))\n",
    "        print(\"Epoch\",i,\"- Train Loss:\",train_loss,\"\\t Test Loss: \",test_loss)\n",
    "\n",
    "    return(W,train_losses,test_losses,train_accuracies,test_accuracies)\n",
    "    # return(w,train_losses,test_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softMax = gradient_descent(XTrain,TTrain,XTest,TTest,\n",
    "                         alpha = .0001,\n",
    "                         lr = .0001,\n",
    "                         epochs = 40,\n",
    "                         batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = np.array(losses)\n",
    "start = 0\n",
    "stop = 40\n",
    "# plt.plot(range(start,stop),softMax[1][start:stop])\n",
    "plt.plot(range(start,stop),softMax[2][start:stop])\n",
    "plt.legend([\"test\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SoftMax, SGD w/ minibatch\")\n",
    "plt.show()\n",
    "plt.plot(range(start,stop),softMax[3][start:stop])\n",
    "plt.plot(range(start,stop),softMax[4][start:stop])\n",
    "plt.legend([\"train\",\"test\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"SoftMax, SGD w/ minibatch\")\n",
    "plt.show()\n",
    "print(softMax[0].shape)\n",
    "print(softMax[0])\n",
    "# print(P(XTest,softMax[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = gradient_descent(trainFeat,trainYears,testFeat,testYears,\n",
    "                         alpha = .0005,\n",
    "                         lr = .0005,\n",
    "                         type = \"count\",\n",
    "                         epochs = 40,\n",
    "                         batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(musicMSE(predict(testFeat,ridge),testYears))\n",
    "print(musicMSE(predict(testFeat,ridge),testYears))\n",
    "print(musicMSE(predict(testFeat,lasso),testYears))\n",
    "print(musicMSE(predict(testFeat,count),testYears))\n",
    "\n",
    "# def errorVec(pred, gt):\n",
    "#     mse = (np.square(np.rint(pred)-gt))\n",
    "#     return mse\n",
    "# print(errorVec(testYears,predict(testFeat,ridge)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b2099187c8c9eb64c67750714a2bd167366c70633164c6614277827fe31c99d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
